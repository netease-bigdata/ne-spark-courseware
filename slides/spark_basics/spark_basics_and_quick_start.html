<html>
  <head>
    <title>Spark概述及快速入门指南</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        ffont-weight: 400;
        margin-bottom: 0;
      }
      .remark-slide-content h1 { font-size: 3em; }
      .remark-slide-content h2 { font-size: 2em; }
      .remark-slide-content h3 { font-size: 1.6em; }
      .footnote {
        position: absolute;
        bottom: 3em;
      }
      li p { line-height: 1.25em; }
      .red { color: #fa0000; }
      .large { font-size: 2em; }
      a, a > code {
        color: rgb(249, 38, 114);
        text-decoration: none;
      }
      code {
        background: #e7e8e2;
        border-radius: 5px;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-code-line-highlighted     { background-color: #373832; }
      .pull-left {
        float: left;
        width: 47%;
      }
      .pull-right {
        float: right;
        width: 47%;
      }
      .pull-right ~ p {
        clear: both;
      }
      #slideshow .slide .content code {
        font-size: 0.8em;
      }
      #slideshow .slide .content pre code {
        font-size: 0.9em;
        padding: 15px;
      }
      .inverse {
        background: #272822;
        color: #777872;
        text-shadow: 0 0 20px #333;
      }
      .inverse h1, .inverse h2 {
        color: #fff;
        line-height: 0.8em;
      }

      /* Slide-specific styling */
      #slide-inverse .footnote {
        bottom: 12px;
        left: 20px;
      }
      #slide-how .slides {
        font-size: 0.9em;
        position: absolute;
        top:  151px;
        right: 140px;
      }
      #slide-how .slides h3 {
        margin-top: 0.2em;
      }
      #slide-how .slides .first, #slide-how .slides .second {
        padding: 1px 20px;
        height: 90px;
        width: 120px;
        -moz-box-shadow: 0 0 10px #777;
        -webkit-box-shadow: 0 0 10px #777;
        box-shadow: 0 0 10px #777;
      }
      #slide-how .slides .first {
        background: #fff;
        position: absolute;
        top: 20%;
        left: 20%;
        z-index: 1;
      }
      #slide-how .slides .second {
        position: relative;
        background: #fff;
        z-index: 0;
      }

      /* Two-column layout */
      .left-column {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
      .left-column h2:last-of-type, .left-column h3:last-of-type {
        color: #000;
      }
      .right-column {
        width: 75%;
        float: right;
        padding-top: 1em;
        font: monospace;
        font-size: 24;
        font-style: all;
      }

      /* Two-column layout inverse*/
      .left-column-inverse {
        color: #777;
        width: 20%;
        height: 92%;
        float: left;
      }
      .left-column-inverse h2:last-of-type, .left-column-inverse h3:last-of-type {
        color: #fff;
      }
      .right-column-inverse {
        color: #fff;
        width: 75%;
        float: right;
        padding-top: 1em;
        font: monospace;
        font-size: 24;
      }
      .right-column-inverse h2, .right-column-inverse h3, .right-column-inverse h4 {
        color: #fff;
      }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle, inverse
name: 标题页

## [NetEase Spark Courses](https://netease-bigdata.github.io/ne-spark-courseware/)

<br>
<br>
<br>
<br>

## Apache Spark概述及快速入门指南

<br>
<br>
<br>
<br>

<img style="zoom: 1.0" src="../../imgs/mammut.png"  align="bottom" />

???
备注：标题 <br>
帮助信息：在网页端按H键可进入帮助页面

---

class: inverse, center
name: agenda

  # Agenda
  ## -
  ** About Me **
  ## -

  ** [什么是Spark？](#4) ** <br>

  ** 正确理解Spark相关概念 ** <br>

  ** 怎么用Spark？ ** <br>

  ** 如何提升Spark技能？ **<br>

---

class: inverse
name: aboutme

.left-column-inverse[
  # About Me
]

.right-column-inverse[
## Kent Yao

2016年11月加入网易，目前在杭州研究院-数据科学中心担任资深大数据平台开发工程师，主导Spark作为核心计算框架在[网易大数据平台](https://bigdata.163yun.com/mammut)的相关研发及大规模应用工作。

前华为技术有限公司大数据技术开发部成员。

<br>

GitHub: https://github.com/yaooqinn

<br>
<br>
<br>

<img style="zoom: 0.618" src="../../imgs/mammut.png" />
]

---

class: inverse
name: sub-agenda

.left-column-inverse[
  # Agenda
  ## [什么是Spark?](#4)
]

.right-column-inverse[
### 从[定义](#5)上理解什么是Spark?
### 从[生态](#6)上理解什么是Spark?
### 从[架构](#7)上理解什么是Spark?
### 从[模块](#8)上理解什么是Spark?
### 从[使用](#9)上理解什么是Spark?
### 从[语言](#10)上理解什么是Spark?
]

---

class: inverse
name: whatisrddoverview
.left-column-inverse[
  ## [什么是Spark?](#4)
  ### 从定义上理解
]

.right-column-inverse[
- In-memory
  - efficient fault tolerance
- Handle inefficiently
  - iterative algorithms
  - interactive data mining tools
- Read-only
  - provides a restricted form of shared memory
  - shared state: coarse-grained transformations(√) fine-grained updates(×).
- A system
  - Spark
]

???
RDD设计背景
在实际应用中，存在许多迭代式算法（比如机器学习、图算法等）和交互式数据挖掘工具，这些应用场景的共同之处是，不同计算阶段之间会重用中间结果，即一个阶段的输出结果会作为下一个阶段的输入。但是，目前的MapReduce框架都是把中间结果写入到HDFS中，带来了大量的数据复制、磁盘IO和序列化开销。虽然，类似Pregel等图计算框架也是将结果保存在内存当中，但是，这些框架只能支持一些特定的计算模式，并没有提供一种通用的数据抽象。RDD就是为了满足这种需求而出现的，它提供了一个抽象的数据架构，我们不必担心底层数据的分布式特性，只需将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，从而避免了中间结果的存储，大大降低了数据复制、磁盘IO和序列化开销。


---

class: inverse
name: whatisrddabstraction
.left-column[
  ## [什么是Spark?](#4)
  ### 从定义上理解
  ### 从生态上理解
]

.right-column[
A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel
- Internally, each RDD is characterized by five main properties:
  - A list of partitions
  - A function for computing each split
  - A list of dependencies on other RDDs
  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)
]

???

一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，
每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，从而可以在集群中的不同节点上进行并行计算。RDD提供了一种高度受限的共享内存模型，即RDD是只读的记录分区的集合，不能直接修改，只能基于稳定的物理存储中的数据集来创建RDD，或者通过在其他RDD上执行确定的转换操作（如map、join和groupBy）而创建得到新的RDD。

---

class:
name: whatisrddlimitations

.left-column[
  ## What is RDD?
  ### Overview
  ### Abstraction
  ### Operations
]

.right-column[
RDDs support two types of operations:
- transformations
  - which create a new dataset from an existing one
- actions
  - which return a value to the driver program after running a computation on the dataset


<img style="zoom: 0.45" src="../imgs/rdd-operations.png" />
<img style="zoom: 0.35" src="../imgs/spark-rddoperations.png" />

]

???

RDD提供了一组丰富的操作以支持常见的数据运算，分为Action和Transformation两种类型，前者用于执行计算并指定输出的形式，后者指定RDD之间的相互依赖关系。

两类操作的主要区别是，transformations（比如map、filter、groupBy、join等）接受RDD并返回RDD，
而action操作（比如count、collect等）接受RDD但是返回非RDD（即输出一个值或结果）。
RDD提供的转换接口都非常简单，都是类似map、filter、groupBy、join等粗粒度的数据转换操作，而不是针对某个数据项的细粒度修改。

---

class:
name: whatisrddlineage

.left-column[
  ## What is RDD?
  ### Overview
  ### Abstraction
  ### Operations
  ### Lineage
]

.right-column[
### A.K.A RDD operator graph or RDD dependency graph
  - a graph of all the parent RDDs of a RDD
  - built as a result of applying transformations to the RDD


<img style="zoom: 0.60" src="../imgs/rdd-lineage.jpg" />

### toDebugString

```scala
scala> val wordCount = sc.textFile("README.md").flatMap(_.split("\\s+")).map((_, 1)).reduceByKey(_ + _)
wordCount: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[21] at reduceByKey at <console>:24

scala> wordCount.toDebugString
res13: String =
(2) ShuffledRDD[21] at reduceByKey at <console>:24 []
 +-(2) MapPartitionsRDD[20] at map at <console>:24 []
    |  MapPartitionsRDD[19] at flatMap at <console>:24 []
    |  README.md MapPartitionsRDD[18] at textFile at <console>:24 []
    |  README.md HadoopRDD[17] at textFile at <console>:24 []
```
]

???

Spark的这种依赖关系设计，使其具有了天生的容错性，大大加快了Spark的执行速度。因为，RDD数据集通过“血缘关系”记住了它是如何从其它RDD中演变过来的，血缘关系记录的是粗颗粒度的转换操作行为，当这个RDD的部分分区数据丢失时，它可以通过血缘关系获取足够的信息来重新运算和恢复丢失的数据分区，由此带来了性能的提升。


---

class:
name: whatisrdddependencies

.left-column[
  ## What is RDD?
  ### Overview
  ### Abstraction
  ### Operations
  ### Lineage
  ### Dependencies
]

.right-column[
### Dependency
  - the base (abstract) class to model a dependency relationship between two or more RDDs.

#### - Narrow Dependencies
  - 1: 1; n: 1
  - allow for pipelined execution

#### - Wide Dependencies
  - 1: n
  - everyday I am shuffling

<img style="zoom: 0.40" src="../imgs/spark-rdd-dependencies.png" />
]

???
窄依赖表现为一个父RDD的分区对应于一个子RDD的分区，或多个父RDD的分区对应于一个子RDD的分区

宽依赖则表现为存在一个父RDD的一个分区对应一个子RDD的多个分区

总体而言，如果父RDD的一个分区只被一个子RDD的一个分区所使用就是窄依赖，否则就是宽依赖。

在两种依赖关系中，窄依赖的失败恢复更为高效，它只需要根据父RDD分区重新计算丢失的分区即可（不需要重新计算所有分区），而且可以并行地在不同节点进行重新计算。而对于宽依赖而言，单个节点失效通常意味着重新计算过程会涉及多个父RDD分区，开销较大。

---

class:
name: whatisrddderddtojob

.left-column[
  ## What is RDD?
  ### Overview
  ### Abstraction
  ### Operations
  ### Lineage
  ### Dependencies
  ### Job Scheduling
]

.right-column[
### Example of how Spark computes job stages.
  - Boxes with solid outlines are RDDs.
  - Partitions are shaded rectangles, in black if they are already in memory.
  - To run an action on RDD G, we build build stages at wide dependencies and pipeline narrow transformations inside each stage.
  - In this case, stage 1’s output RDD is already in RAM, so we run stage 2 and then 3

<img style="zoom: 0.60" src="../imgs/spark-rddtojob.png" />
]

???

Spark通过分析各个RDD的依赖关系生成了DAG，再通过分析各个RDD中的分区之间的依赖关系来决定如何划分阶段，具体划分方法是：在DAG中进行反向解析，遇到宽依赖就断开，遇到窄依赖就把当前的RDD加入到当前的阶段中；将窄依赖尽量划分在同一个阶段中，可以实现流水线计算

---

class:
name: whatisspark

.left-column[
  ## What is Spark?
  ### Overview
]
.right-column[

##### Apache Spark is a fast and general-purpose cluster computing system
- #### Speed
  - DAG execution engine supporting acyclic data flow and in-memory computing

- #### Ease of Use
  - Write applications quickly in Java, Scala, Python, R, SQL

- #### Generality
  - Combine SQL, streaming, and complex analytics.

- #### Runs Everywhere
  - Standalone
  - Mesos
  - Yarn
]

---

class:
name:sparkarchitecture

.left-column[
  ## What is Spark?
  ### Overview
  ### Architecture
]
.right-column[
  ![](../imgs/spark-stack.png)
- Spark SQL
  - support for structured data and relational queries
- Spark Streaming
  - processing real-time data streams
- MLlib
  - built-in machine learning library
- GraphX
  -  graph processing

]

---

class:
name:clusteroverview

.left-column[
  ## What is Spark?
  ### Overview
  ### Architecture
  ### Components
]
.right-column[
  .bottom[![](../imgs/cluster-overview.png)]
- Driver program
    - the process running the main() function of the application and creating the SparkContext
- Cluster manager
    - an external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)
- Worker node
    - any node that can run application code in the cluster
- Executor
    - a process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors.
]

???

Spark运行架构包括集群资源管理器（Cluster Manager）、运行作业任务的工作节点（Worker Node）、每个应用的任务控制节点（Driver）和每个工作节点上负责具体任务的执行进程（Executor）。其中，集群资源管理器可以是Spark自带的资源管理器，也可以是YARN或Mesos等资源管理框架。
与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点：一是利用多线程来执行具体的任务（Hadoop MapReduce采用的是进程模型），减少任务的启动开销；二是Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算时，可以将中间结果存储到这个存储模块里，下次需要时，就可以直接读该存储模块里的数据，而不需要读写到HDFS等文件系统里，因而有效减少了IO开销；或者在交互式查询场景下，预先将表缓存到该存储系统上，从而可以提高读写IO性能。

---

class:
name:whatissparksparkcontext

.left-column[
  ## What is Spark?
  ### Overview
  ### Architecture
  ### Components
  ### SparkContext
]
.right-column[
- the entrance / the heart / the master of a Spark APP

![](../imgs/sparkcontext-services.png)
]

---

class:
name: sparkprogramingguide

.left-column[
  ## Spark Programming Guide
  ### Terminology
]

.right-column[
- Spark application
    - a driver program that runs the user’s main function and executes various parallel operations on a cluster
- RDD
    - a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel
- Shared variables
    - broadcast variables
    - accumulators
- Job
    - a parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect)
- Stage
    - each job gets divided into smaller sets of tasks called stages that depend on each other (similar to the map and reduce stages in MapReduce)
- Task
    - a unit of work that will be sent to one executor
]

???

在Spark中，一个应用（Application）由一个任务控制节点（Driver）和若干个作业（Job）构成，一个作业由多个阶段（Stage）构成，一个阶段由多个任务（Task）组成。当执行一个应用时，任务控制节点会向集群管理器（Cluster Manager）申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行任务，运行结束后，执行结果会返回给任务控制节点，或者写到HDFS或者其他数据库中。

---

class:
name:sparkprogramingguidesparkshell

.left-column[
  ## Spark Programming Guide
  ### Terminology
  ### Spark Shell
]
.right-column[
#### - The launch code
```shell
./bin/spark-shell
```

#### - A simple case: Word Count
```scala
scala> val lines = spark.read.textFile(“README.md”)
lines: org.apache.spark.sql.Dataset[String] = [value: string]
scala> val words = lines.flatMap(_.split(“ ”))
words: org.apache.spark.sql.Dataset[String] = [value: string]
scala> val groups = words.groupByKey(identity)
groups: org.apache.spark.sql.KeyValueGroupedDataset[String,String] = org.apache.spark.sql.KeyValueGroupedDataset@6473ef35
scala> val wordcount = groups.count
wordcount: org.apache.spark.sql.Dataset[(String, Long)] = [value: string, count(1): bigint]
```

#### - Simpler way
```scala
val res = spark.read.textFile("README.md")
  .flatMap(_.split(" "))
  .groupByKey(identity)
  .count
  .collect
```
]

---

class:
name:sparkprogramingguidesparksubmit1

.left-column[
  ## Spark Programming Guide
  ### Terminology
  ### Spark Shell
  ### Spark Submit
]
.right-column[
#### Self-Contained Word Count App
#####  - Maven dependency on Spark
```xml
<properties>
  <spark.version>2.1.0</spark.version>
</properties>

<dependencies>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>${spark.version}</version>
    <scope>provided</scope>
  </dependency>
</dependencies>
```
]

---

class:
name:sparkprogramingguidesparksubmit2

.left-column[
  ## Spark Programming Guide
  ### Terminology
  ### Spark Shell
  ### Spark Submit
]
.right-column[
#### Self-Contained Word Count App
#####  - Write main procedure
```scala
package com.netease.mammut.spark.training

import org.apache.spark.{SparkConf, SparkContext}

object WordCount {

  def main(args: Array[String]): Unit = {
    require(args.length == 1, "Usage: WordCount <input file>")

    val conf = new SparkConf().setAppName("Word Count").setMaster("local[*]")
    val sparkContext = new SparkContext(conf)

    val textFile = sparkContext.textFile(args(0), 2)
    val words = textFile.flatMap(_.split(" "))
    val ones = words.map((_, 1))
    val counts = ones.reduceByKey(_ + _)

    val res = counts.collect()

    for ((word, count) <- res) {
      println(word + ": " + count)
    }

    sparkContext.stop()
  }
}
```
]

---

class:
name:sparkprogramingguidesparksubmit3

.left-column[
  ## Spark Programming Guide
  ### Terminology
  ### Spark Shell
  ### Spark Submit
]
.right-column[
#### Self-Contained Word Count App
#####  - Compiling & Packaging
```scala
<build>
  <plugins>
    <plugin>
      <!-- https://mvnrepository.com/artifact/net.alchim31.maven/scala-maven-plugin -->
     <groupId>net.alchim31.maven</groupId>
      <artifactId>scala-maven-plugin</artifactId>
      <version>3.2.2</version>
      <executions>
        <execution>
          <id>scala-compile-first</id>
          <goals>
            <goal>compile</goal>
          </goals>
        </execution>
      </executions>
    </plugin>
  </plugins>
</build>
```

```shell
mvn package
```
]

---

class:
name:sparkprogramingguidesparksubmit4

.left-column[
  ## Spark Programming Guide
  ### Terminology
  ### Spark Shell
  ### Spark Submit
]
.right-column[
#### Self-Contained Word Count App
#####  - Submit Application
###### - spark-submit
```shell
./bin/spark-submit \
  --class <main-class> \
  --master <master-url> \
  --deploy-mode <deploy-mode> \
  --conf <key>=<value> \
  ... # other options
  <application-jar> \
  [application-arguments]
```
###### - our example
```shell
./bin/spark-submit
  --class com.netease.mammut.spark.training.WordCount
  /some/path/to/mammut-spark-training-1.0-SNAPSHOT.jar
  README.md
```
]

---

class:
name: sparkprogramingguide

.left-column[
  ## Spark Programming Guide
  ### Terminology
  ### Spark Shell
  ### Spark Submit
  ### What's More
]
.right-column[
  .center.middle[
  ### Where to Go from Here
  Click [Spark Programing Guide](http://spark.apache.org/docs/2.1.0/programming-guide.html) to see more advantage usages of RDDs.
  ]
]


---

class: inverse
name: agenda

.left-column-inverse[
  # Agenda
  ### Spark Core
  ### Spark SQL
]

.right-column-inverse[
#### What is Spark SQL?
#### Spark SQL Programming Guide
]

---

class:
name: sparksqldataset

.left-column[
  ## Spark SQL
  ### Dataset/DataFrame
]
.right-column[

    <img style="zoom: 1.0" src="../imgs/rdd.png" />
]

???

Spark 1.x的RDD更多意义上是一个一维、只有行概念的数据集，比如 RDD[Person]，那么一行就是一个Person，存在内存里也是把 Person 作为一个整体（序列化前的java object，或序列化后的bytes）。

Spark 2.x里，一个 Person 的 Dataset 或 DataFrame，是二维行+列的数据集，比如一行一个 Person，有 name:String, age:Int, height:Double 三列；在内存里的物理结构，也会显式区分列边界。
Dataset/DataFrame 在 API 使用上有区别：Dataset 相比 DataFrame 而言是 type-safe 的，能够在编译时对 AnalysisExecption 报错（如下图示例）:
Dataset/DataFrame 存储方式无区别：两者在内存中的存储方式是完全一样的、是按照二维行列（UnsafeRow）来存的，所以在没必要区分 Dataset 或 DataFrame 在 API 层面的差别时，我们统一写作 Dataset/DataFrame

---

class:
name: sparksql

.left-column[
  ## Spark SQL


]
.right-column[
<img style="zoom: 0.40" src="../imgs/sparksqlcatalyst.png" />
]

---

class: middle, center, inverse
name: greetings
# Thank You!
### [Kent Yao]

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.


    </textarea>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create({
        ratio: '16:9',
        slideNumberFormat: 'Slide %current% of %total%',
        // .. or by using a format function
        slideNumberFormat: function (current, total) {
          return ' ' + current + ' of ' + total;
        },
        highlightLanguage: 'scala',
        highlightStyle: 'monokai',
        highlightLines: true,
        // arta, ascetic, dark, default, far, github, googlecode, idea, ir-black, magula, monokai, rainbow, solarized-dark, solarized-light, sunburst, tomorrow, tomorrow-night-blue, tomorrow-night-bright, tomorrow-night, tomorrow-night-eighties, vs, zenburn
        highlightStyle: 'zenburn'
      });
    </script>
  </body>
</html>
